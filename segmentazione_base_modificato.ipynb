{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92fe6cf",
   "metadata": {},
   "source": [
    "### Installazione e Importazione delle Librerie\n",
    "In questa sezione, installeremo e importeremo tutte le librerie necessarie per la segmentazione delle immagini. Le librerie includono strumenti per la manipolazione dei dati, la costruzione e l'addestramento del modello, e l'analisi dei risultati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986112f-a0fa-4c1e-ba94-14ae0575a54d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U torch torchvision matplotlib pillow tqdm opencv-python albumentations torchsummary segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcc714-3e32-4fa8-9fe8-12df2dee78a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importare le librerie necessarie\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchvision import models, transforms as T\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import time\n",
    "from torchsummary import summary\n",
    "\n",
    "!pip install -q segmentation-models-pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0180c",
   "metadata": {},
   "source": [
    "### Preprocessamento delle Immagini\n",
    "Questa sezione descrive il preprocessamento delle immagini, che include il ridimensionamento, la normalizzazione e la suddivisione del dataset in set di addestramento e di test. Questi passaggi sono cruciali per garantire che i dati siano in un formato adatto per l'addestramento del modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff4621-1018-4b7b-82e3-7be3a50d6368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Definizione dei percorsi dei dati\n",
    "#IMAGE_PATH = 'immagini/images/validation/'\n",
    "#MASK_PATH = 'immagini/annotations/validation/'\n",
    "IMAGE_PATH = 'immagini3/images/'\n",
    "MASK_PATH = 'immagini3/annotations/'\n",
    "IMAGE_PATH2 = 'immagini5/images/'\n",
    "MASK_PATH2 = 'immagini5/annotations/'\n",
    "\n",
    "risoluz_vert = 192\n",
    "risoluz_orizz = 384\n",
    "\n",
    "\n",
    "n_classes = 30  #150 Numero delle classi nel dataset ADEChallengeData2016\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b4931-6e2b-4dd4-b6b2-ecfb62a06cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_df():\n",
    "    name = []\n",
    "    for dirname, _, filenames in os.walk(IMAGE_PATH):\n",
    "        for filename in filenames:\n",
    "            name.append(filename.split('.')[0])\n",
    "    \n",
    "    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n",
    "\n",
    "df = create_df()\n",
    "print('Total Images: ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184ad99-648b-4723-b772-22f1d55d51ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split data\n",
    "X_trainval, X_test = train_test_split(df['id'].values, test_size=0.1, random_state=19)\n",
    "X_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n",
    "\n",
    "print('Train Size   : ', len(X_train))\n",
    "print('Val Size     : ', len(X_val))\n",
    "print('Test Size    : ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35a0ad-4b21-41a7-b399-7b4ab938cbbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = Image.open(IMAGE_PATH + df['id'][100] + '.jpg')\n",
    "mask = Image.open(MASK_PATH + df['id'][100] + '.png')\n",
    "print('Image Size', np.asarray(img).shape)\n",
    "print('Mask Size', np.asarray(mask).shape)\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.imshow(mask, alpha=0.6)\n",
    "plt.title('Picture with Mask Applied')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2f798-5670-4c09-8435-c6b5477bdd61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DriveDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "        self.patches = patch\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + '.jpg')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + '.png', cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug['image'])\n",
    "            mask = aug['mask']\n",
    "        \n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "        \n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
    "        img = t(img)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        if self.patches:\n",
    "            img, mask = self.tiles(img, mask)\n",
    "            \n",
    "        return img, mask\n",
    "    \n",
    "    def tiles(self, img, mask):\n",
    "\n",
    "        img_patches = img.unfold(1, 512, 512).unfold(2, 768, 768) \n",
    "        img_patches  = img_patches.contiguous().view(3,-1, 512, 768) \n",
    "        img_patches = img_patches.permute(1,0,2,3)\n",
    "        \n",
    "        mask_patches = mask.unfold(0, 512, 512).unfold(1, 768, 768)\n",
    "        mask_patches = mask_patches.contiguous().view(-1, 512, 768)\n",
    "        \n",
    "        return img_patches, mask_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8c9e2-2e3e-40c0-bac2-2715cac5961c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "t_train = A.Compose([A.Resize(risoluz_vert, risoluz_orizz, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(), A.VerticalFlip(), \n",
    "                     A.GridDistortion(p=0.2), A.RandomBrightnessContrast((0,0.5),(0,0.5)),\n",
    "                     A.GaussNoise()])\n",
    "\n",
    "t_val = A.Compose([A.Resize(risoluz_vert, risoluz_orizz, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(),\n",
    "                   A.GridDistortion(p=0.2)])\n",
    "\n",
    "#datasets\n",
    "train_set = DriveDataset(IMAGE_PATH, MASK_PATH, X_train, mean, std, t_train, patch=False)\n",
    "val_set = DriveDataset(IMAGE_PATH, MASK_PATH, X_val, mean, std, t_val, patch=False)\n",
    "\n",
    "#dataloader\n",
    "batch_size= 3 \n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac71eee",
   "metadata": {},
   "source": [
    "### Definizione del Modello\n",
    "In questa parte, definiremo l'architettura del modello di deep learning utilizzato per la segmentazione delle immagini. Discuteremo delle scelte di architettura e delle funzioni di perdita utilizzate per l'addestramento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79386da-06cc-43e3-b069-8f0c71a3726f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=n_classes, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bd155-3d94-4de1-ae66-f590c6bcf700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = smp.DeepLabV3Plus('resnet50', encoder_weights='imagenet', classes=n_classes, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b341367a-4842-4fcb-ba13-906f0e52b330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = smp.FPN('efficientnet-b0', encoder_weights='imagenet', classes=n_classes, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc7545-8849-4a12-99b1-71c6b3d23cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = smp.Unet('resnet34', encoder_weights='imagenet', classes=n_classes, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967c3ab-426e-464f-b8ac-069ac7b23d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'UNet-ResNet'\n",
    "\n",
    "# Carica il modello\n",
    "model = torch.load(model_name + '.pt')\n",
    "\n",
    "# Metti il modello in modalità di valutazione\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8da1d",
   "metadata": {},
   "source": [
    "### Training del Modello\n",
    "Qui dettagliamo il processo di addestramento del modello, inclusi i parametri di addestramento, le tecniche di ottimizzazione, e il monitoraggio delle metriche di prestazione durante l'addestramento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf16f95-25e1-4c7f-b61a-cdbe497a7617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd20242-b86e-4a03-b91e-aeaf60efbbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=n_classes):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7691c-858a-42ce-af78-0d280aa1eb64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_iou = []; val_acc = []\n",
    "    train_iou = []; train_acc = []\n",
    "    lrs = []\n",
    "    min_loss = np.inf\n",
    "    decrease = 1 ; not_improve=0\n",
    "\n",
    "    model.to(device)\n",
    "    fit_time = time.time()\n",
    "    for e in range(epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0\n",
    "        iou_score = 0\n",
    "        accuracy = 0\n",
    "        #training loop\n",
    "        model.train()\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            #training phase\n",
    "            image_tiles, mask_tiles = data\n",
    "            if patch:\n",
    "                bs, n_tiles, c, h, w = image_tiles.size()\n",
    "\n",
    "                image_tiles = image_tiles.view(-1,c, h, w)\n",
    "                mask_tiles = mask_tiles.view(-1, h, w)\n",
    "            \n",
    "            image = image_tiles.to(device); mask = mask_tiles.to(device);\n",
    "            #forward\n",
    "            output = model(image)\n",
    "            loss = criterion(output, mask)\n",
    "            #evaluation metrics\n",
    "            iou_score += mIoU(output, mask)\n",
    "            accuracy += pixel_accuracy(output, mask)\n",
    "            #backward\n",
    "            loss.backward()\n",
    "            optimizer.step() #update weight          \n",
    "            optimizer.zero_grad() #reset gradient\n",
    "            \n",
    "            #step the learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            scheduler.step() \n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        else:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_accuracy = 0\n",
    "            val_iou_score = 0\n",
    "            #validation loop\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(tqdm(val_loader)):\n",
    "                    #reshape to 9 patches from single image, delete batch size\n",
    "                    image_tiles, mask_tiles = data\n",
    "\n",
    "                    if patch:\n",
    "                        bs, n_tiles, c, h, w = image_tiles.size()\n",
    "\n",
    "                        image_tiles = image_tiles.view(-1,c, h, w)\n",
    "                        mask_tiles = mask_tiles.view(-1, h, w)\n",
    "                    \n",
    "                    image = image_tiles.to(device); mask = mask_tiles.to(device);\n",
    "                    output = model(image)\n",
    "                    #evaluation metrics\n",
    "                    val_iou_score +=  mIoU(output, mask)\n",
    "                    test_accuracy += pixel_accuracy(output, mask)\n",
    "                    #loss\n",
    "                    loss = criterion(output, mask)                                  \n",
    "                    test_loss += loss.item()\n",
    "            \n",
    "            #calculatio mean for each batch\n",
    "            train_losses.append(running_loss/len(train_loader))\n",
    "            test_losses.append(test_loss/len(val_loader))\n",
    "\n",
    "\n",
    "            if min_loss > (test_loss/len(val_loader)):\n",
    "                print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss/len(val_loader))))\n",
    "                min_loss = (test_loss/len(val_loader))\n",
    "                decrease += 1\n",
    "                if decrease % 5 == 0:\n",
    "                    print('saving model...')\n",
    "                    torch.save(model, 'Unet-Mobilenet_v2_mIoU-{:.3f}.pt'.format(val_iou_score/len(val_loader)))\n",
    "                    \n",
    "\n",
    "            if (test_loss/len(val_loader)) > min_loss:\n",
    "                not_improve += 1\n",
    "                min_loss = (test_loss/len(val_loader))\n",
    "                print(f'Loss Not Decrease for {not_improve} time')\n",
    "                if not_improve == 7:\n",
    "                    print('Loss not decrease for 7 times, Stop Training')\n",
    "                    break\n",
    "            \n",
    "            #iou\n",
    "            val_iou.append(val_iou_score/len(val_loader))\n",
    "            train_iou.append(iou_score/len(train_loader))\n",
    "            train_acc.append(accuracy/len(train_loader))\n",
    "            val_acc.append(test_accuracy/ len(val_loader))\n",
    "            print(\"Epoch:{}/{}..\".format(e+1, epochs),\n",
    "                  \"Train Loss: {:.3f}..\".format(running_loss/len(train_loader)),\n",
    "                  \"Val Loss: {:.3f}..\".format(test_loss/len(val_loader)),\n",
    "                  \"Train mIoU:{:.3f}..\".format(iou_score/len(train_loader)),\n",
    "                  \"Val mIoU: {:.3f}..\".format(val_iou_score/len(val_loader)),\n",
    "                  \"Train Acc:{:.3f}..\".format(accuracy/len(train_loader)),\n",
    "                  \"Val Acc:{:.3f}..\".format(test_accuracy/len(val_loader)),\n",
    "                  \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
    "        \n",
    "    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n",
    "               'train_miou' :train_iou, 'val_miou':val_iou,\n",
    "               'train_acc' :train_acc, 'val_acc':val_acc,\n",
    "               'lrs': lrs}\n",
    "    print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7675521c-3c36-4da8-a56c-2f7db5aae909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a9490-ebcd-4962-8060-a1dee9c832ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_lr = 1e-3\n",
    "epoch = 25\n",
    "weight_decay = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n",
    "                                            steps_per_epoch=len(train_loader))\n",
    "\n",
    "history = fit(epoch, model, train_loader, val_loader, criterion, optimizer, sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29750a21-2ad4-4faa-967f-b11a0d02c4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'Unet-Resnet2_160m_25ep.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4385bd4-3f2e-4e69-acaf-c5ba9bff1a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history['val_loss'], label='val', marker='o')\n",
    "    plt.plot( history['train_loss'], label='train', marker='o')\n",
    "    plt.title('Loss per epoch'); plt.ylabel('loss');\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_score(history):\n",
    "    plt.plot(history['train_miou'], label='train_mIoU', marker='*')\n",
    "    plt.plot(history['val_miou'], label='val_mIoU',  marker='*')\n",
    "    plt.title('Score per epoch'); plt.ylabel('mean IoU')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_acc(history):\n",
    "    plt.plot(history['train_acc'], label='train_accuracy', marker='*')\n",
    "    plt.plot(history['val_acc'], label='val_accuracy',  marker='*')\n",
    "    plt.title('Accuracy per epoch'); plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aaa4cf-38c8-4d51-b2de-0cad955a12c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plot_score(history)\n",
    "plot_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75dec29-6fe2-4308-af6c-98768b620f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_loss(history, filename='loss.png'):\n",
    "    plt.plot(history['val_loss'], label='val', marker='o')\n",
    "    plt.plot(history['train_loss'], label='train', marker='o')\n",
    "    plt.title('Loss per epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def plot_score(history, filename='score.png'):\n",
    "    plt.plot(history['train_miou'], label='train_mIoU', marker='*')\n",
    "    plt.plot(history['val_miou'], label='val_mIoU', marker='*')\n",
    "    plt.title('Score per epoch')\n",
    "    plt.ylabel('mean IoU')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def plot_acc(history, filename='accuracy.png'):\n",
    "    plt.plot(history['train_acc'], label='train_accuracy', marker='*')\n",
    "    plt.plot(history['val_acc'], label='val_accuracy', marker='*')\n",
    "    plt.title('Accuracy per epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14cf46-82e0-47b7-a86d-dee892a5b83d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plot_score(history)\n",
    "plot_acc(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb4df5",
   "metadata": {},
   "source": [
    "### Analisi dei Risultati\n",
    "Analizziamo i risultati del modello. Questa sezione include la valutazione delle metriche di prestazione, visualizzazioni dei risultati della segmentazione e un'analisi complessiva delle prestazioni del modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0ffe7-b70e-447f-9e7d-6799d95e5914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DriveTestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_path, mask_path, X, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + '.jpg')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + '.png', cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug['image'])\n",
    "            mask = aug['mask']\n",
    "        \n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "        \n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "\n",
    "t_test = A.Resize(risoluz_vert, risoluz_orizz, interpolation=cv2.INTER_NEAREST)\n",
    "test_set = DriveTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef79e4-769d-4b5b-8af5-2ad2255328ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DriveTestDataset2(Dataset):\n",
    "    def __init__(self, img_path, mask_path, X, mean, std, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + '.jpg')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + '.png', cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = aug['image']\n",
    "            mask = aug['mask']\n",
    "        \n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
    "        img = t(img)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return img, mask\n",
    "    \n",
    "# Caricamento del dataset di test\n",
    "t_test = A.Compose([A.Resize(risoluz_vert, risoluz_orizz, interpolation=cv2.INTER_NEAREST)])\n",
    "test_set2 = DriveTestDataset2(IMAGE_PATH, MASK_PATH, X_test, mean, std, transform=t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fec8f-9d5a-49ee-81ba-6b0fd7c60f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_image_mask_miou(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    model.eval()\n",
    "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "    image = t(image)\n",
    "    model.to(device); image=image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        \n",
    "        output = model(image)\n",
    "        score = mIoU(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7e5bcc-1642-4aef-9bdc-53f1578d6c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_image_mask_pixel(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    model.eval()\n",
    "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "    image = t(image)\n",
    "    model.to(device); image=image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        \n",
    "        output = model(image)\n",
    "        acc = pixel_accuracy(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705fe7e-4a24-4562-b7bc-4e8633ab45ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def miou_score(model, test_set):\n",
    "    score_iou = []\n",
    "    for i in tqdm(range(len(test_set))):\n",
    "        img, mask = test_set[i]\n",
    "        pred_mask, score = predict_image_mask_miou(model, img, mask)\n",
    "        score_iou.append(score)\n",
    "    return score_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81258d3f-3840-45f4-852e-97a6b4de88a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mob_miou = miou_score(model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4431e-3708-4fa5-9fc8-8602912cce45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pixel_acc(model, test_set):\n",
    "    accuracy = []\n",
    "    for i in tqdm(range(len(test_set))):\n",
    "        img, mask = test_set[i]\n",
    "        pred_mask, acc = predict_image_mask_pixel(model, img, mask)\n",
    "        accuracy.append(acc)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a165e10-6104-4405-8d45-f3fc3900d617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mob_acc = pixel_acc(model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c5f4f-57fc-4821-8c15-030cf7193827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Funzione per misurare i tempi di inferenza\n",
    "def measure_inference_time(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(data_loader):\n",
    "            images = images.to(device)\n",
    "            start_time = time.time()\n",
    "            _ = model(images)\n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "    avg_inference_time = total_time / len(data_loader.dataset)\n",
    "    return total_time, avg_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae4f6d8-c28f-475b-8ba6-c4912d84b95f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataloader per il test set\n",
    "test_loader = DataLoader(test_set2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Misurazione dei tempi di inferenza\n",
    "total_inference_time, avg_inference_time = measure_inference_time(model, test_loader, device)\n",
    "print(f'Tempo medio di inferenza per immagine: {avg_inference_time:.4f} secondi')\n",
    "print(f'Tempo totale di inferenza: {total_inference_time:.4f} secondi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5c28c-cdf0-491c-a92f-160138c1464a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Test Set mIoU', np.mean(mob_miou))\n",
    "print('Test Set Pixel Accuracy', np.mean(mob_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb420db-50c2-4b0d-8c69-ae238520ca2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_comparison_masks(test_set, model):\n",
    "    indices = [0, 40, 200]  # Indici per la prima, quinta e ultima immagine\n",
    "    fig, axes = plt.subplots(len(indices), 3, figsize=(20, len(indices) * 5))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        image, mask = test_set[idx]\n",
    "        pred_mask, miou_score = predict_image_mask_miou(model, image, mask)\n",
    "        pred_mask_pixel, pixel_accuracy = predict_image_mask_pixel(model, image, mask)\n",
    "        \n",
    "        axes[i, 0].imshow(image)\n",
    "        axes[i, 0].set_title('Picture')\n",
    "        axes[i, 0].set_axis_off()\n",
    "\n",
    "        axes[i, 1].imshow(mask)\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].set_axis_off()\n",
    "\n",
    "        axes[i, 2].imshow(pred_mask)\n",
    "        axes[i, 2].set_title(f'{model_name} | mIoU {miou_score:.3f} | Pixel Accuracy {pixel_accuracy:.3f}')\n",
    "        axes[i, 2].set_axis_off()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_comparison_masks(test_set, model, save_path='comparison_masks'):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    indices = [0, 40, 200]  # Indici per la prima, quinta e ultima immagine\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        image, mask = test_set[idx]\n",
    "        pred_mask, miou_score = predict_image_mask_miou(model, image, mask)\n",
    "        pred_mask_pixel, pixel_accuracy = predict_image_mask_pixel(model, image, mask)\n",
    "\n",
    "        # Salva l'immagine originale\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.imshow(image)\n",
    "        plt.title('Picture')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout(pad=1)  # Riduce il padding attorno all'immagine\n",
    "        plt.savefig(f'{save_path}/{model_name}_image_{i+1}.png', bbox_inches='tight', pad_inches=0.1)\n",
    "        plt.close()\n",
    "\n",
    "        # Salva il Ground Truth\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.imshow(mask)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout(pad=1)  # Riduce il padding attorno all'immagine\n",
    "        plt.savefig(f'{save_path}/{model_name}_ground_truth_{i+1}.png', bbox_inches='tight', pad_inches=0.1)\n",
    "        plt.close()\n",
    "\n",
    "        # Salva la maschera predetta\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.imshow(pred_mask)\n",
    "        plt.title(f'{model_name} | mIoU {miou_score:.3f} | Pixel Accuracy {pixel_accuracy:.3f}')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout(pad=1)  # Riduce il padding attorno all'immagine\n",
    "        plt.savefig(f'{save_path}/{model_name}_pred_mask_{i+1}.png', bbox_inches='tight', pad_inches=0.1)\n",
    "        plt.close()\n",
    "        \n",
    "def save_predicted_masks_with_metrics(test_set, model, mod, save_path='predicted_masks'):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    indices = [0, 40, 200]  # Indici per la prima, quinta e ultima immagine\n",
    "    if mod == \"quantized\":\n",
    "        modifica = \"Quantized\"\n",
    "    elif mod == \"pruned\":\n",
    "        modifica = \"Pruned\"\n",
    "    else:\n",
    "        modifica = \"Quantized & Pruned\"\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        image, mask = test_set[idx]\n",
    "        pred_mask, miou_score = predict_image_mask_miou(model, image, mask)\n",
    "        pred_mask_pixel, pixel_accuracy = predict_image_mask_pixel(model, image, mask)\n",
    "\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.imshow(pred_mask)\n",
    "        plt.title(f'{modifica} {model_name} | mIoU {miou_score:.3f} | Pixel Accuracy {pixel_accuracy:.3f}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout(pad=1)  # Riduce il padding attorno all'immagine\n",
    "        plt.savefig(f'{save_path}/{model_name}_predicted_mask_{mod}_{i+1}.png', bbox_inches='tight', pad_inches=0.1)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae0052-71bd-465e-ba51-2106d7273af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_comparison_masks(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f82cc0-6b25-4a12-a114-5fc2ac1a4b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_comparison_masks(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec30bb8-d210-4606-9680-7cd9197b98d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_segmentation(model, dataloader, num_classes):\n",
    "    model.eval()\n",
    "    precisions = np.zeros(num_classes)\n",
    "    recalls = np.zeros(num_classes)\n",
    "    f1s = np.zeros(num_classes)\n",
    "    n_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, true_masks in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            true_masks = true_masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            preds = preds.cpu().numpy().flatten()\n",
    "            true_masks = true_masks.cpu().numpy().flatten()\n",
    "\n",
    "            for cls in range(num_classes):\n",
    "                cls_mask = (true_masks == cls)\n",
    "                cls_pred = (preds == cls)\n",
    "                \n",
    "                precision = precision_score(cls_mask, cls_pred, zero_division=0)\n",
    "                recall = recall_score(cls_mask, cls_pred, zero_division=0)\n",
    "                f1 = f1_score(cls_mask, cls_pred, zero_division=0)\n",
    "\n",
    "                precisions[cls] += precision\n",
    "                recalls[cls] += recall\n",
    "                f1s[cls] += f1\n",
    "\n",
    "            n_samples += 1\n",
    "\n",
    "    precisions /= n_samples\n",
    "    recalls /= n_samples\n",
    "    f1s /= n_samples\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        print(f'Class {cls}: Precision = {precisions[cls]:.4f}, Recall = {recalls[cls]:.4f}, F1-score = {f1s[cls]:.4f}')\n",
    "\n",
    "    return precisions, recalls, f1s\n",
    "\n",
    "# Esempio di utilizzo\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# num_classes = 10  # Adatta questo numero alle tue classi\n",
    "precisions, recalls, f1s = evaluate_segmentation(model, test_loader, n_classes)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4dcc4-90cd-4bf9-92e7-161350a0c1a7",
   "metadata": {},
   "source": [
    "### Quantizzazione e Pruning\n",
    "Questa parte copre le tecniche di quantizzazione e pruning utilizzate per ottimizzare il modello, rendendolo più efficiente durante l'inferenza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e82c4e-0743-4f07-9e53-04818db962e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Funzione per quantizzare un layer lineare o convoluzionale con quantizzazione non lineare\n",
    "def nonlinear_quantize_layer(layer, num_bits):\n",
    "    quantized_layer = layer\n",
    "    if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "        # Calcola i parametri di quantizzazione non lineare\n",
    "        abs_max = torch.abs(layer.weight).max()\n",
    "        scale = abs_max / (2 ** (num_bits - 1) - 1)\n",
    "        quantized_weight = torch.clamp(layer.weight, -abs_max.item(), abs_max.item())\n",
    "        quantized_weight = torch.round(quantized_weight / scale)\n",
    "        quantized_weight = torch.clamp(quantized_weight, -2**(num_bits-1), 2**(num_bits-1) - 1)\n",
    "\n",
    "        quantized_layer.weight.data = quantized_weight\n",
    "        if layer.bias is not None:\n",
    "            quantized_layer.bias.data = torch.round(layer.bias.data / scale)\n",
    "    return quantized_layer\n",
    "\n",
    "# Funzione per quantizzare un intero modello con quantizzazione non lineare\n",
    "def nonlinear_quantize_model(model, num_bits):\n",
    "    quantized_model = model\n",
    "    for name, layer in quantized_model.named_children():\n",
    "        if isinstance(layer, nn.Sequential) or isinstance(layer, nn.ModuleList):\n",
    "            quantized_model.add_module(name, nonlinear_quantize_model(layer, num_bits))\n",
    "        elif isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            quantized_model.add_module(name, nonlinear_quantize_layer(layer, num_bits))\n",
    "        else:\n",
    "            quantized_model.add_module(name, layer)\n",
    "    return quantized_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd695310-a04f-468f-af0d-c0923b0e8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantizza il modello di segmentazione semantica\n",
    "model_quantized = copy.deepcopy(model)\n",
    "model_quantized = nonlinear_quantize_model(model_quantized, num_bits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613cfbe-e51c-40d1-a28f-c812816eb426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size\n",
    "\n",
    "# compare the sizes\n",
    "f=print_size_of_model(model,\"Original\")\n",
    "q=print_size_of_model(model_quantized,\"Quantized\")\n",
    "if f > q:\n",
    "    print(\"{0:.2f} times smaller\".format(f/q))\n",
    "elif f < q:\n",
    "    print(\"{0:.2f} times larger\".format(q/f))\n",
    "else:\n",
    "    print(\"Same size\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f491eb-2e74-4e05-a7e8-22899af3a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for name, param in model_quantized.named_parameters():\n",
    "    print(name, param.dtype)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979ebb4-7e06-436f-bc9e-043e173d04f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "# Funzione per applicare il pruning strutturato\n",
    "def structured_prune_model(model, amount):\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.ln_structured(module, name='weight', amount=amount, n=1, dim=0)\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "            prune.ln_structured(module, name='weight', amount=amount, n=1, dim=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb680c0-95f6-471a-8ab7-6cac9f17c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicazione del pruning strutturato\n",
    "model_pruned = copy.deepcopy(model)\n",
    "model_quantized_pruned = copy.deepcopy(model_quantized)\n",
    "\n",
    "model_pruned = structured_prune_model(model_pruned, amount=0.03)\n",
    "model_quantized_pruned = structured_prune_model(model_quantized_pruned, amount=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0cf65f-e2da-4312-9516-7bcfaaae1be9",
   "metadata": {},
   "source": [
    "### Analisi dei Risultati dopo Quantizzazione\n",
    "Analizziamo i risultati del modello dopo la quantizzazione. Questa sezione include la valutazione delle metriche di prestazione, visualizzazioni dei risultati della segmentazione e un'analisi complessiva delle prestazioni del modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487ca80-f7c7-434d-84de-fb9d7119d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_miou_quantized = miou_score(model_quantized, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a007843-fa75-467c-bcdd-9f6e6dc69570",
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_acc_quantized = pixel_acc(model_quantized, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744e038-0887-4222-be20-ab3e4e83f16b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Test Set mIoU', np.mean(mob_miou_quantized))\n",
    "print('Test Set Pixel Accuracy', np.mean(mob_acc_quantized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9a889-c51d-4c3d-ada0-eb92edca6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misurazione dei tempi di inferenza\n",
    "total_inference_time, avg_inference_time = measure_inference_time(model_quantized, test_loader, device)\n",
    "print(f'Tempo medio di inferenza per immagine: {avg_inference_time:.4f} secondi')\n",
    "print(f'Tempo totale di inferenza: {total_inference_time:.4f} secondi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f88a4-637d-43d1-9ffc-fcb04d07d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = \"quantized\"\n",
    "save_predicted_masks_with_metrics(test_set, model_quantized, mod)\n",
    "show_comparison_masks(test_set, model_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860fbfc-0cd7-49df-a5a1-418761654611",
   "metadata": {},
   "source": [
    "### Analisi dei Risultati dopo Pruning\n",
    "Analizziamo i risultati del modello dopo il pruning. Questa sezione include la valutazione delle metriche di prestazione, visualizzazioni dei risultati della segmentazione e un'analisi complessiva delle prestazioni del modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc98e2-b30d-49e0-a057-b2f92cd91b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_miou_pruned = miou_score(model_pruned, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47baec28-6f5e-42f5-b6f3-13e0ad6ea8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_acc_pruned = pixel_acc(model_pruned, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4f4ec-5aff-4cf1-9ae6-17339680d8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Test Set mIoU', np.mean(mob_miou_pruned))\n",
    "print('Test Set Pixel Accuracy', np.mean(mob_acc_pruned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae602e-3b36-4d03-83f0-54d775d44fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misurazione dei tempi di inferenza\n",
    "total_inference_time, avg_inference_time = measure_inference_time(model_pruned, test_loader, device)\n",
    "print(f'Tempo medio di inferenza per immagine: {avg_inference_time:.4f} secondi')\n",
    "print(f'Tempo totale di inferenza: {total_inference_time:.4f} secondi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4fa81-42e7-4e2b-a497-4b7830daeb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = \"pruned\"\n",
    "save_predicted_masks_with_metrics(test_set, model_pruned, mod)\n",
    "show_comparison_masks(test_set, model_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e5338-a47a-4d53-a7c8-7649b24576cb",
   "metadata": {},
   "source": [
    "### Analisi dei Risultati dopo Quantizzazione e Pruning\n",
    "Infine, analizziamo i risultati del modello dopo quantizzazione e pruning. Questa sezione include la valutazione delle metriche di prestazione, visualizzazioni dei risultati della segmentazione e un'analisi complessiva delle prestazioni del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4701236-2925-4d73-af73-a317a7e90535",
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_miou_quantized_pruned = miou_score(model_quantized_pruned, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223fa0b4-88e6-4843-a1c7-b0b78151b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_acc_quantized_pruned = pixel_acc(model_quantized_pruned, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33c5ee-128e-45e7-8815-a06eabf13815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Test Set mIoU', np.mean(mob_miou_quantized_pruned))\n",
    "print('Test Set Pixel Accuracy', np.mean(mob_acc_quantized_pruned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98125a80-2ea1-4e7a-84b2-c155dc49bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misurazione dei tempi di inferenza\n",
    "total_inference_time, avg_inference_time = measure_inference_time(model_quantized_pruned, test_loader, device)\n",
    "print(f'Tempo medio di inferenza per immagine: {avg_inference_time:.4f} secondi')\n",
    "print(f'Tempo totale di inferenza: {total_inference_time:.4f} secondi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c5f51f-fad3-4c4f-98fc-b468d861a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = \"quantized_pruned\"\n",
    "save_predicted_masks_with_metrics(test_set, model_quantized_pruned, mod)\n",
    "show_comparison_masks(test_set, model_quantized_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d527d4-7144-4b95-8d7c-89d3fbe13d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
